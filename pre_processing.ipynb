{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VS Code에서 메소드 파라미터 확인 방법 : ctrl+shift+space\n",
    "# Jupyter Notebook에서 메소드 파라미터 확인 방법 : shift+tab+tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 공통 라이브러리, 세팅 ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기|복사 -----------------------------------------------------------\n",
    "# 주요 encoding 방법 : encoding = 'utf8', 'euc-kr', 'cp949'\n",
    "\n",
    "df = pd.read_csv('bike.csv')    # 원본 데이터는 한번만 불러서 복사해 사용\n",
    "   \n",
    "p1 = df.copy()  # 원본 데이터 복사 (문제번호 : p1, p2...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA ----------------------------------------------------------------------------\n",
    "\n",
    "df.info()                # 데이터, 결측치 갯수, 컬럼별 데이터 타입 확인 \n",
    "df                       # 각 컬럼 데이터 확인\n",
    "df.describe()            # 기본 통계값 (이상치 판별시 유용), 컬럼수 많은 경우 df.describe().T 유용\n",
    "df.isna().sum()          # 컬럼별|행별 결측값 갯수  , axis=0|1 지정 가능 | df.isnull().sum()\n",
    "df.notna().sum()         # 컬럼별|행별 비결측값 갯수, axis=0|1 지정 가능 | df.notnull().sum()\n",
    "df.count()               # 컬럼별|행별 비결측값 갯수, axis=0|1 지정 가능\n",
    "df.mode()                # 최빈값\n",
    "\n",
    "list1.count('text')      # 리스트에 지정 문자열 전체가 포함된 요소의 갯수 (리스트만 가능)\n",
    "df['col1'].str.len()     # 지정 컬럼의 문자열 갯수\n",
    "df['col'].str.count('')  # 컬럼에서 문자 갯수 반환 (1개 많게 반환됨에 주의)\n",
    "df['col'].str.count('a') # 컬럼에서 'a'가 포함된 횟수 반환\n",
    "\n",
    "df.columns               # 컬럼명 반환\n",
    "df.dtypes                # 컬럼별 자료형 (int, float 등)\n",
    "type(data)               # 자료형 반환 (int, float, pandas.core.frame.DataFrame 등)\n",
    "\n",
    "series1.unique()         # 고유값 반환 (시리즈만 가능!!!) \n",
    "series1.nunique()        # 고유값 갯수 (시리즈만 가능!!!) \n",
    "\n",
    "df[['col1', 'col2']].corr()[['col1']]         # 상관계수 반환, 'pearson'(디폴트),'spearman','kendall'\n",
    "df.groupby('col1')[['col2', 'col3']].corr()   # 그룹별 상관계수 반환 [★★★★★]\n",
    "\n",
    "import missingno as msno # 결측치를 시각화해서 보여주는 라이브러리\n",
    "msno.matrix(df)          # msno.matrix(df.sample(500)) 데이터갯수가 많은 경우 샘플링도 가능\n",
    "msno.bar(df)             # 결측치를 bar 그래프 형태로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성 및 변환 --------------------------------------------------------------------\n",
    "# 문자열의 리스트 생성방법 2가지는 결과가 다르니 주의!!!\n",
    "l1 = list('abcde')  # ['a', 'b', 'c', 'd', 'e']\n",
    "l2 = ['abcde']      # ['abcde']\n",
    "\n",
    "# 데프 생성\n",
    "df = pd.DataFrame({'col1': [1,2,...], 'col2': [3,4,..]})   # data는 한 개라도 리스트|시리즈 형태로 입력 [★★★★★]\n",
    "df = pd.DataFrame(array1, columns = list1)                 # 컬럼명 변경시 columns = df.columns+'_S' 등, columns= 필수 \n",
    "se = pd.Series([리스트])                                   # 시리즈 생성\n",
    "\n",
    "# 더미 변수 컬럼 생성 (옵션 : drop_first=True)\n",
    "df1 = pd.get_dummies(df)                      # 컬럼 미지정시 모든 명목형 변수 컬럼의 더미 변수 생성\n",
    "df1 = pd.get_dummies(df, columns=['col1'])    # 원 데이터에 더미 변수 컬럼 병합 생성 (컬럼 지정시 columns= 필수)\n",
    "df1 = pd.get_dummies(df['col1'])              # 지정 컬럼의 더미 변수 컬럼만 별도 생성, 원 데이터에 병합 필요 \n",
    "\n",
    "# Categorical 데이터 처리\n",
    "df['col'] = pd.Categorical(df['col']) # 범주형 변수로 변환\n",
    "df['who'].cat.codes                   # 범주형 데이터를 수치형으로 변환해 줌. 'A' -> 0,, 'B' -> 1 ...\n",
    "df['who'].cat.categories              # 범주형 데이터의 카테고리들을 출력\n",
    "\n",
    "# 자료형 변환\n",
    "int(value1)                   # 하나의 값을 정수로 변경\n",
    "df['col'].astype(int)         # 컬럼의 자료형을 정수로 변경\n",
    "list1   = data1.to_list()     # array|시리즈 > 리스트 (시험장용 버전은 tolist()이니 주의!!!)\n",
    "dict1   = series.to_dict()    # 시리즈 > 딕셔너리\n",
    "dict1 = {k:v for k,v in zip(l3, range(0,7))}  # 연령대 리스트와 라벨 번호를 대응시킨 딕셔너리 생성\n",
    "series1 = pd.Series(list1, index= X.columns)  # 리스트 > 시리즈 (스칼라 연산시 필요)\n",
    "# .astype('int')의 경우 숫자가 아닌 다른 값이 있으면 에러가 발생하지만\n",
    "# pd.to_numeric() : int, float 등 적당한 것으로 알아서 변환\n",
    "df['term'] = pd.to_numeric(df['term'], errors = 'coerce')  # errors='coerce' : 잘못된 문자열은 NaN 반환\n",
    "df['Price_Log'] = np.log1p(df['Price']) \n",
    "# np.log1p()는 log(1+x) 값을 계산하므로, x가 0이라도 로그 값 계산 가능\n",
    "\n",
    "# 컬럼 1개(시리즈) -> 데프로 변환 방법 3가지 (머신러닝 X값 생성시 필수!!!)\n",
    "df1 = series1.to_frame()      # .to_frame() 이용 \n",
    "df1 = df.loc[: ,['A']]        # 리스트 이용\n",
    "df1 = df.iloc[:, 0:1]         # 슬라이싱 이용\n",
    "\n",
    "# 리스트 -> 데프로 변환 방법\n",
    "df = pd.DataFrame(list1, columns = ['col1','col2'])\n",
    "\n",
    "# 수치형 컬럼에 원치 않는 값이 포함되었을 경우 확인 후 수치형으로 변환 방법\n",
    "df['age'].mean()  # TypeError: 23493485... to numeric  -> 'age'가 숫자가 아니라는 의미\n",
    "df['age']//10*10  # TypeError: unsupported operand type(s) for //: 'str' and 'int' -> 'age'가 'str'이라는 의미\n",
    "df['age'] = df['age'].astype(int)   # ValueError: invalid literal for int() with base 10: '*82' -> * 확인 가능\n",
    "df['age'] = df['age'].str.replace('*','').astype(int)   # 데이터의 *를 제거하고, int로 변환\n",
    "\n",
    "# 문자열 변환\n",
    "df['ID'] = df['ID'].str.split('[').str[0]              # split후 반드시!!! .str[] 인덱싱\n",
    "df1 = df['Hobbies'].str.split(';', expand=True)        # expand=True를 해주면 별도 컬럼으로 분리되어 생성\n",
    "df['title'] = df['name'].replace([리스트], '문자열')   # 복수개의 문자열을 단일 문자열로 변경 가능\n",
    "\n",
    "# 문자열 정규식\n",
    "df['title'] = df['name'].extract(r'([A-Za-z]+)\\.')     # 마침표(.) 앞의 알파벳 문자열 반환\n",
    "# r: Raw 문자열 표기 방식. 백슬래시(\\)를 이스케이프 문자로 처리하지 않도록 함\n",
    "# ([A-Za-z]+): [ ]괄호 부분은 capturing group으로 대소문자 알파벳이 하나 이상 반복(+)되는 패턴을 의미\n",
    "# \\.: 마침표(.)를 문자 그대로 의미 (마침표(.)는 \\가 없으면 1개 임의 문자를 의미)\n",
    "\n",
    "# datetime 객체 변환 (format=\"%Y-%m-%d\"은 읽어들이는 데이터 형태를 지정하는 것임의 주의!!!)\n",
    "pd.to_datetime(df['Date'])  # datetime 객체로 변환 (2000-01-02)  [★★★★★]\n",
    "pd.to_datetime(df['Date'], format=\"%Y-%m-%d\") # datetime 객체로 변환 (포맷 지정)\n",
    "pd.to_datetime(df['Date'], errors = 'coerce') # errors = 'coerce' : 잘못된 문자열은 NaN으로 반환\n",
    "df['date'].dt.year           # 년 반환 (2023)\n",
    "df['date'].dt.month          # 달 반환 (01)\n",
    "df['date'].dt.month_name()   # 달명 반환 (January~) 괄호 사용에 주의!!!\n",
    "df['date'].dt.week           # 주 반환 \n",
    "df['Date'].dt.isocalendar()  # 연도, 주차, 요일 3개 값을 한번에 반환, .week : 주차수 반환, (월:1 ~ 일:7)\n",
    "df['date'].dt.weekday        # 요일 반환 (월:0 ~ 일:6)  [★★★★★]\n",
    "df['date'].dt.dayofweek      # 요일 반환 (월:0 ~ 일:6) \n",
    "df['date'].dt.day_name()     # 요일명 반환 (monday~),  괄호 사용에 주의!!!\n",
    "df['date'].dt.date           # 날짜 반환 (02)\n",
    "df['date'].dt.dayofyear      # 연중 몇 번째 날 반환 \n",
    "df['date'].dt.hour           # 시간 반환\n",
    "\n",
    "df['date'] - pd.DateOffset(days=5)          # 년~나노초 가능 (PD데이트없애)\n",
    "df['date'] + pd.to_timedelta(5, unit='day') # 주~나노초 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼명|index명 조작 ------------------------------------------------------------\n",
    "\n",
    "df1 = df.reset_index(drop=True)    # 인덱스 리셋 (drop=True : 리셋 전 index 삭제)\n",
    "df1 = df.set_index('date')         # 지정 컬럼을 index로, 멀티인덱스도 가능 ['year','month']\n",
    "df1.index.name = 'My_Index'        # index 객체명 (None 지정시 제거)\n",
    "df1.columns.name = 'My_Columns'    # columns 객체명 (None 지정시 제거) (index 위치, 컬럼명과 동일 행) \n",
    "\n",
    "l1 = (df.columns+'_S').to_list()                 # 기존 컬럼명에 문자열 추가한 리스트 생성 (시험장 버전은 tolist())\n",
    "df1.columns = df1.columns.str.replace(' ','-')   # 컬럼명의 빈칸을 '-'로 변경\n",
    "df1.columns = [column_list]                      # 컬럼 갯수 적을 경우 .rename 보다 편리 (부분 적용 불가)\n",
    "df = df.rename(columns = {'old': 'new'})         # columns= 필수, 딕셔너리 입력\n",
    "df = df.rename(index   = {0: '2022-09'})         #   index= 필수, 딕셔너리 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데프 선택 : 인덱스 지정 --------------------------------------------------------\n",
    "# 끝 숫자 포함여부 혼동이 될 수 있으니, 슬라이싱 시는 .iloc 사용 (.loc 말고!!!)\n",
    "\n",
    "# 행 지정 + 컬럼 지정 선택 (실수를 줄이기 위해, 컬럼 지정시 loc 이용해 컬럼명을 직접 입력 하자!!!)\n",
    "df.loc[행, 열]           # 숫자, 문자, Boolean 사용 가능 (슬라이싱시 end 포함 -> 반드시 반환 값 체크!!!)\n",
    "df.loc[: 'A']            # 모든 행 + 컬럼 1개 (시리즈)\n",
    "df.loc[['A', 'B'], :]    # 행 지정 + 모든 컬럼 선택 (모든 컬럼)\n",
    "\n",
    "df.iloc[행, 열]          # 숫자만 사용 가능 (슬라이싱시 end 포함 안함)\n",
    "df.iloc[:, 0]            # 모든 행 + 컬럼 1개 (시리즈)\n",
    "df.iloc[:,::2]           # 모든 행 + 짝수번째 컬럼만 선택\n",
    "df.iloc[[29, 30],  :]    # 행 지정 + 모든 컬럼 선택 (모든 컬럼)\n",
    "\n",
    "# 행 슬라이싱 지정 + 모든 컬럼 선택  \n",
    "df1 = df[1:5]            # df[1]과 같이 단일 숫자만 입력시는 KeyError 발생\n",
    "\n",
    "# 행 무작위 + 모든 컬럼 선택 : .sample() \n",
    "df1 = df.sample(n=100, random_state=123).reset_index()       # 샘플 갯수로 선택\n",
    "df1 = df.sample(frac=0.3, random_state=123).reset_index()    # 샘플 비율로 선택\n",
    "\n",
    "df1 = df.groupby('season').sample(n=20, random_state=123)    # 층화 추출, 전체에 random_state 적용\n",
    "df1 = df.groupby('season').apply(lambda x: x.sample(n=20))   # 층화 추출, 그룹별 random_state 적용\n",
    "\n",
    "# 행 일정 간격 단위 + 모든 컬럼 선택 : .resample() : 'W'(1주), '2W'(2주), 'M'(월) 등 지정 가능\n",
    "# 반드시 Index가 DatetimeIndex, TimedeltaIndex, PeriodIndex 만 가능\n",
    "df_w = df.resample('W').sum()     # 주 단위 샘플링 (sum(), max(), min() 등 지정 가능)\n",
    "\n",
    "# 컬럼만 지정 + 모든 행 선택\n",
    "df1 = df['A']                     # 컬럼 1개 (시리즈)\n",
    "df1 = df[['A','B']]               # 컬럼 2개 (데프)\n",
    "df1 = df.select_dtypes(['int'])   # dtypes별 컬럼 선택, 'float','int','object' 등 (리스트로 입력!!!)\n",
    "\n",
    "\n",
    "# 2. 데프 선택 : 조건 판단 ----------------------------------------------------------\n",
    "# for loop 사용시에는 선택 결과를 동일한 데프명에 입력을 해야만 컬럼별 선택결과가 누적이 되니 주의!!!\n",
    "\n",
    "# 2-1. 연산자 비교 (==, != , > , >=, <, <=)\n",
    "c1[c1!=1]                                     # 행+렬 전체를 반환하되, 조건에 맞지 않는 값은 NaN 반환\n",
    "df1 = df.loc[(df > 0).sum(axis=1) >= 1]       # 옆 방향으로 0 보다 큰 값이 2개 이상 있는 행 반환\n",
    "df1 = df.loc[(df['col'].str[-2:]=='1')]       # 선택 행 반환\n",
    "df1 = df.loc[df['col']=='1']                  # 선택 행 반환\n",
    "df1 = df.loc[df['col']=='1'].index            # 선택 행 index 반환\n",
    "df1 = df.loc[df['col']=='1','TEMP']           # 선택 행의 지정 컬럼 값 반환 \n",
    "df1 = df.loc[df['col']=='1','TEMP'].max()     # 선택 행의 지정 컬럼의 함수값 반환\n",
    "\n",
    "# 2-2. 유무 확인 + 전체 요소 : .isin(리스트)\n",
    "df1 = df.loc[df['col'].isin(['01','15'])]     # '01' 또는 '15'가 포함된 행 반환\n",
    "df1 = df.loc[df['col'].isin(range(1,11))]     # range()와 조합해 1~10 사이값이 포함된 행 반환 (중요!!!)\n",
    "\n",
    "for col in p1.columns:\n",
    "   p1 = p1.loc[p1[col].isin(['Yes','No'])]    # 컬럼별로 지정값 포함하는 행만 반환 (동일 데프명 사용 주의!!!)\n",
    "\n",
    "# 2-3. 유무 확인 + 문자열 일부 : .str.contains('문자열') \n",
    "df1 = df.loc[df['date'].str.contains('15')]     # 지정값 포함 행 반환\n",
    "df1 = df.loc[df['date'].str.contains('01|15')]  # 따옴표는 요소별로 쓰지 않고, 전체에 쓰는 것에 주의!!!\n",
    "df['Hobbies'].str.contains('^A')     # A 로 시작하는 문자열 포함 여부 반환\n",
    "df['Hobbies'].str.contains('A$')     # A 로 끝나는 문자열 포함 여부 반환\n",
    "df['Hobbies'].str.contains('[AB]')   # A 또는 B를 포함하는 문자열 포함 여부 반환\n",
    "\n",
    "# 2-4. 유무 확인 + 문자열 일부 시작/끝 : .str.startswith(), .str.endswith() \n",
    "df1 = df.loc[df['date'].str.startswith('15')]   # 지정값으로 시작하는 행 반환 (s가 붙었음에 유의!!!)\n",
    "df1 = df.loc[df['date'].str.endswith('15')]     # 지정값으로 끝나는 행 반환 (s가 붙었음에 유의!!!)\n",
    "\n",
    "# 2-5. 유무 확인 + 결측치 : .isna(), .notna()\n",
    "df1 = df.loc[df['col'].isna()]                # 지정 컬럼에 결측치 포함된 행 반환\n",
    "df1 = df.loc[df['col'].notna()]               # 지정 컬럼에 결측치 포함되지 않은 행 반환\n",
    "df1 = df.loc[~(df.isna().any(axis=1))]        # 옆 방향, 결측치가   1개 이상 포함된 행 제외하고 반환 (중요!!!)\n",
    "df1 = df.loc[~(df.isna().sum(axis=1) >= n)]   # 옆 방향, 결측치가   n개 이상 포함된 행 제외하고 반환 (중요!!!)\n",
    "df1 = df.loc[df.notna().any(axis=1)]          # 옆 방향, 비결측치가 1개 이상 포함된 행 반환 (중요!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행|컬럼 삭제 ---------------------------------------------------------------------\n",
    "\n",
    "df1 = df.drop([0,1])                    # 행 삭제   : 리스트 (default : axis=0)\n",
    "df1 = df.drop(df.index[0:2])            # 행 삭제   : 인덱스로 슬라이싱\n",
    "df1 = df.drop(['A','B'], axis=1)        # 컬럼 삭제 : 리스트\n",
    "df1 = df.drop(df.columns[0:2], axis=1)  # 컬럼 삭제 : 컬럼명으로 슬라이싱 \n",
    "\n",
    "df.duplicated(keep='first')             # 중복행 유무를 bool값으로 반환\n",
    "df1 = df.drop_duplicates('col')         # 지정 컬럼 기준 중복행 삭제 (keep='first'|'last')\n",
    "\n",
    "df1 = df.dropna()                       # 결측치 포함 행 삭제, thresh=n (행의 결측치가 n개 이상일 경우 삭제)\n",
    "df1 = df.dropna(subset=['col1','col2']) # 결측치 포함 컬럼이 복수개일 경우 컬럼 지정해서 결측치 포함 행 삭제\n",
    "df1 = df.dropna(axis=1, how='any')      # 결측치 포함 컬럼 삭제, how='any'|'all'(전부 결측치이면 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 병합 --------------------------------------------------------------------\n",
    "\n",
    "# concat은 index 기준으로 병합되므로 concat 전에 reset_index() 필수!!!\n",
    "df3 = pd.concat([df1, df2], axis=1).reset_index()    # 2개 이상, 옆|아래 방향, 조건 부여 불가 join='inner'|'outer'\n",
    "df3 = pd.concat([df1, df2], ignore_index= True)      # 아래 방향 병합, 인덱스 reset필요시 ignore_index= True 지정\n",
    "df3 = pd.concat([df1, df2], join='inner')            \n",
    "\n",
    "# merge()  병합\n",
    "'''\n",
    "- 병합시 기준이 되는 컬럼을 기준으로 4가지 병합 옵션 : how = 'left', 'right', 'outer', 'inner'\n",
    "- 주로 많이 사용하는 옵션은 how='left' \n",
    "- merge는 2개의 데프만 병합 가능 -> 3개 이상을 병합 하고자 할 때는, 2개씩 순차적으로 병합\n",
    "- 병합시 기준이 되는 컬럼의 내용은 동일한데 단지 컬럼명만 다르다면\n",
    "- left_on='성함', right_ont='이름' 처럼 해당 컬럼병을 지정해 주어야 한다.'''\n",
    "df3 = pd.merge(df1, df2).reset_index()               # 자동으로 공통 컬럼을 기준으로 병합, 2개만, 옆 방향만, 조건 부여 가능\n",
    "df3 = pd.merge(df1, df2, left_on='col1',right_on='col2',how='left').reset_index()  # 옵션 지정 \n",
    "\n",
    "# 데프에 행 추가 방법 : 신규 데프로 생성 후 기존 데프에 병합\n",
    "df2 = pd.DataFrame({'DATE':['09-30','09-31'],'TEMP':[None]*2})     # 1. 신규 데프 생성\n",
    "df3 = pd.concat([df1, df2]).reset_index()                          # 2. 기존 데프의 마지막 행으로 병합\n",
    "df3 = pd.concat([df1.iloc[:2], df2, df1.iloc[2:]]).reset_index()   # 2. 기존 데프의 중간에 병합도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 컬럼 값 대체|새로운 컬럼에 입력  ------------------------------------------\n",
    "# 결측치 값 : None(파이썬 내장 객체), np.NaN(numpy 객체), pd.NA(pandas 객체)\n",
    "# for loop에서  컬럼1 = 컬럼1,  데프1 = 데프1(행 선택) 매우 주의!!!\n",
    "\n",
    "# 1개 값 대체|입력 : 선택컬럼+모든 행에...\n",
    "df[\"ID\"] = df[\"ID\"].str[1:]                           # .str 문자열화 > 슬라이싱 \n",
    "df['ID'] = df['ID'].str.split('[').str[0]             # split후 반드시!!! .str[] 인덱싱\n",
    "df['ID'] = df['ID'].apply(lambda x: x.split('[')[0])  # .apply()는 .str 없이 split 후, .str 없이 인덱싱 가능\n",
    "\n",
    "df['ID']  = df['ID']*(max-min) + min                  # MinMax 변환된 데이터 역변환\n",
    "df['ID']  = df['ID']*std1 + mean1                     # Standars 변환된 데이터 역변환\n",
    "df['sum'] = df.sum(axis=1)                            # 옆 방향 합계 값으로 새로운 컬럼 생성\n",
    "\n",
    "df['Band'] = df['Age']//10*10                         # 나이를 연령대로 변환 (수치형 10,  20, ...)\n",
    "df['Band'] = (df['Age']//10*10).astype(str)+'s'       # 나이를 연령대로 변환 (범주형 10s, 20s, ...)\n",
    "\n",
    "# 1개 값 대체|입력 :  선택 행+선택/신규 컬럼 | 선택 행+모든 컬럼\n",
    "df.loc[df['date'] =='15', 'date']   = '보름'          # 선택 행 + 컬럼 선택해 값 대체\n",
    "df.loc[df['date'] =='15', 'date_N'] = '보름'          # 선택 행 + 신규 컬럼 생성 입력 (매우 중요!!!) \n",
    "df.loc[df['date'] =='15'] = 30                        # 선택 행 + 모든 컬럼 동일 값 대체 (주의!!!)\n",
    "\n",
    "# 1개 값 대체 : 데프 전체|선택 컬럼 + 결측치만...\n",
    "df = df.fillna(999)                                   # 데프 전체 결측치를 단일 값 대체\n",
    "df.fillna(method='ffill').fillna(method='bfill')      # 데프 전체 결측치 직전 값, 첫 행 결측치만 직후 값으로 채움\n",
    "\n",
    "df['col'] = df['col'].fillna(df['col'].mean())        # 단일 컬럼 결측치 + 단일 값 대체\n",
    "df.loc[df['col'].isna(),'col'] = df['col'].mean()     # 단일 컬럼 결측치 + 단일 값 대체\n",
    "\n",
    "# 2개 값 + 1, 0 대체|입력 : .astype('int')\n",
    "df['col_N'] = (df['col'] =='1').astype(int)           # Boolean 값 > 1(True)|0(False) 반환\n",
    "\n",
    "# 2개 값 + 임의값 입력 대체|입력 : np.where\n",
    "df['col_N'] = np.where(df['col'] =='1', 99, 0)        \n",
    "\n",
    "# 3개 이상 + 임의값 대체|입력 + N:1 지정\n",
    "df.loc[df['col'].isin(list1), 'col_N'] = '상'\n",
    "df.loc[df['col'].isin(list2), 'col_N'] = '중'\n",
    "df.loc[df['col'].isin(list3), 'col_N'] = '하'\n",
    "\n",
    "# 3개 이상 + 임의값 대체|입력 + 조건별:1 지정\n",
    "df.loc[(df['col']<5), 'col_N'] = '하'\n",
    "df.loc[(5<=df['col'])&(df['col']<7), 'col_N'] = '중'\n",
    "df.loc[(7<=df['col'])&(df['col']<10), 'col_N'] = '상'\n",
    "\n",
    "# 3개 이상 + 임의값 대체 + 1:1 지정 + 그 외 값은 그대로 반환\n",
    "df = df.replace({'High':5,'Mid':3,'Low':1})                 # 데프|시리즈 가능, 요소 전체 매칭 대체 (딕셔너리 !!!)\n",
    "df[\"ID\"] = df[\"ID\"].replace({'M':'m','L':'l'}, regex=True)  # 데프|시리즈 가능, 해당 문자열 대체 (regex=True 지정!!!)\n",
    "df[\"ID\"] = df[\"ID\"].str.replace('M','m')                    #    시리즈만 가능, 해당 문자열 1개만 대체 가능 (딕셔너리 아님!!!)          \n",
    "\n",
    "# 3개 이상 + 임의값 대체 + 1:1 지정 + 그 외 값은 NaN 반환\n",
    "df['col'] = df['col'].map({'Yes':1,'No':0})             # 시리즈만 가능, 데프에는 for loop 필요!!! \n",
    "df['col'] = df['col'].map({'Yes':1,'No':0}).fillna(-1)  # fillna(), dropna() 적용 가능\n",
    "\n",
    "# 'Yes'는 1, 'No'는 0로 대체하고, 그 외의 값을 갖는 행은 삭제하는 사례\n",
    "for col in p3.columns:\n",
    "    p3[col] = p3[col].map({'Yes':1, 'No':0})   # 지정 외의 값은 NaN 반환\n",
    "p3 = p3.dropna()                               # for loop 완료 이후 전체를 dropna()\n",
    "\n",
    "\n",
    "# categorical data의 결측치 대체 ----------------------------------------------------------------\n",
    "# .dtype 확인시 categorical data인 경우 fillna()하면 에러 발생\n",
    "# 먼저 category를 추가해 준 다음 .fillna()를 진행해야 함\n",
    "df1['deck'].dtype  # CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G'], ordered=False)\n",
    "df1['deck'] = df1['deck'].cat.add_categories('No Data').fillna('No Data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정렬 ---------------------------------------------------------------------------\n",
    "# sort_values() 후 인덱스를 사용 할 경우에는, 반드시 reset_index() 필요!!!\n",
    "df1 = df.sort_values('price', ascending=True).reset_index(drop=True)                    # 데프|시리즈 값 기준 정렬\n",
    "df1 = df.sort_values(['price','cut'], ascending=[True, False]).reset_index(drop=True)   # 데프|시리즈 값 기준 정렬\n",
    "df1 = df.sort_index()                                                                   # index 기준 정렬\n",
    "\n",
    "list1 = [(3,30), (4,20), (2,50)]   # list 요소가 복수일 경우 첫번째 값 기준으로 정렬 됨!!!  \n",
    "sorted(list1, reverse=True)        # list 정렬, 내림차순 : reverse=True (원본 데이터는 변경 없음)\n",
    "list1.sort(reverse=True)           # list 정렬, 내림차순 : reverse=True (원본 데이터가 변경 됨)\n",
    "\n",
    "df.loc[::-1, :]   # 행의 순서를 역순으로 변경\n",
    "df.loc[:, ::-1]   # 컬럼의 순서를 역순으로 변경\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby() ) 그룹화 + 간단한 함수 적용시 유용 ------------------------------------------------     : \n",
    "# groupby('col')은 마치 'col'의 유일값을 기준으로 만들어진 각 그룹을 별도의 데프로 취급\n",
    "\n",
    "# groupby() 후 연산시 \n",
    "#1. 메소드 적용 가능한 경우 -> 메소드 사용\n",
    "#2. 메소드 적용 불가한 경우 -> 아래의 3가지를 이용\n",
    "df.groupby('col1')['col2'].apply()      # 단일 결과값 반환시 사용\n",
    "df.groupby('col1')['col2'].agg()        # 복수 결과값 반환시 사용\n",
    "df.groupby('col1')['col2'].transform()  # group별 index가 아닌 원래 컬럼의 index로 반환\n",
    "\n",
    "# lambda x 사용 방법\n",
    "# lambda x의 x는 'col'의 유일값을 기준으로 만들어진 각 데이터프레임을 의미\n",
    "# lambda 는 단일 결괏값만 반환 -> 복수 결과값을 반환하려면 복수의 lambda 식을 리스트|튜플로 지정 필요\n",
    "\n",
    "# 적용 가능한 함수들\n",
    "# count( ) : 결측값 제외한 갯수 반환\n",
    "# cumsum( ), cumcount( ) : 누적합, 누적 count 반환\n",
    "# describe( ), mean( ), median( ), max( ), min( )\n",
    "# first( ), last( ) : 결측값 제외한 첫번째 값, 마지막 값 반환\n",
    "# prod( ), size( ), std( ), sum( ), value_counts( ),var( ) 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "||                                       |     .agg()    |     .apply()   |   선호 메소드      |\n",
    "|:---------:| :------------------------: | :-----------: | :------------: | :----------------: |\n",
    "|단일값 반환| 단일 함수 연산             |      가능     |      가능      |   apply            |\n",
    "|단일값 반환| 단일 컬럼간 연산           |      가능     |      가능      |   apply            |\n",
    "|단일값 반환| 복수 컬럼간 연산           |      불가     |      가능      |   apply            |\n",
    "|복수값 반환| 단일 컬럼에 복수 함수 적용 |  컬럼 분리 O  |   컬럼 분리 X  |   agg([리스트])    |\n",
    "|복수값 반환| 복수 컬럼에 각각 함수 적용 |  컬럼 분리 O  |   컬럼 분리 X  |   agg({딕셔너리})  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['A','B'])[['D','E']].sum()                  # 복수개의 컬럼 -> 멀티인덱스 반환\n",
    "df.groupby(['A','B'], as_index=False)[['D','E']].sum()  # as_index=False 지정시 일반 인덱스로 반환\n",
    "df.groupby(['A','B'])[['D','E']].sum().reset_index()    # .reset_index() 적용후 일반 인덱스로 반환\n",
    "\n",
    "# groupby() 컬럼에 추가 조작 필요시 df['col'] 형태로 입력 필요 (예) df['age']//10*10, df['date'].dt.week)\n",
    "df.groupby(df['age']//10*10)['bmi'].mean()             # 연령대별로 그룹화하여 'bmi'의 평균값 반환\n",
    "df['cum_rain'] = df.groupby('area')['rain'].cumsum()   # groupby 후 함수 계산값으로 새로운 컬럼 생성\n",
    "\n",
    "# .groupby().unstack() : 그룹화 계산결과를 crosstab 형태의 데프로 반환!!!\n",
    "df.groupby('area')['col1'].mean().unstack()                 \n",
    "\n",
    "\n",
    "# .apply() 사용례 (복수 컬럼간 연산은 .apply()만 가능!!!)\n",
    "# .apply()는 groupby 객체의 컬럼을 받을 수도 있으며, groupby 객체 전체를 받을 수도 있음\n",
    "#   .agg()는 groupby 객체의 컬럼을 받을 수는 있으나, groupby 객체 전체를 받을 수는 없음\n",
    "df.groupby('area').apply(lambda x: x['col1'].max())         # groupby 객체 전체를 받을 수 있음\n",
    "df.groupby('area')['col1'].apply(lambda x: x.max())         # groupby 객체의 컬럼을 받을 수 있음\n",
    "df.groupby(df['date'].dt.week).apply(lambda x: x.mean())    # 주단위 그룹핑 후 함수 계산\n",
    "df.groupby('area')['col1'].apply(lambda x: x.max()-x.min())             # 단일 컬럼간 연산  \n",
    "df.groupby('area').apply(lambda x: x['col1'].max()-x['col1'].min())     # 단일 컬럼간 연산  \n",
    "df.groupby('area').apply(lambda x: x['col1'].max()-x['col2'].min())     # 복수 컬럼간 연산\n",
    "df.groupby('area').apply(lambda x: x)                                   # 그룹 생성 후 전체 데프 반환 (중요!!!)\n",
    "df.groupby('area').apply(lambda x: x.sort_values('col1'))               # 그룹 + 정렬 후 전체 데프 반환\n",
    "df.groupby('area').apply(lambda x: [x['col1'].max(), x['col2'].min()])  # 복수 컬럼별 함수 적용 (두 개 값이 1개 컬럼으로 반환)\n",
    "                                                                        # .apply(lambda x: [  ,  ]) 형태에 유의!!!\n",
    "df[['col1', 'col2']] = df['col'].apply(pd.Series)  # 복수 데이터를 갖는 하나의 컬럼을 데이터별 컬럼으로 분리하는 방법\n",
    "\n",
    "\n",
    "# .agg() 사용예 : 'min', 'max' 등 간단한 함수는 바로 적용, 바로 대응이 안 될 경우 lambda 함수 이용\n",
    "df.groupby('area')['col1'].agg(['max','min'])                              # 단일 컬럼 + 복수 함수 (리스트)\n",
    "df.groupby('area')['col1'].agg([('최대값','max'),('최소값','min')])        # 튜플로 각각의 컬럼명 지정 가능\n",
    "df.groupby('area')['col1'].agg([np.mean, np.sum])                          # numpy 통계함수 적용시는 '' 사용 않음\n",
    "df.groupby('area')['col1'].agg(['max', lambda x: x.quantile(0.75)])        # 단일 컬럼에 함수 + 람다식 (리스트)\n",
    "df.groupby('area')['col1'].agg(['max','min', lambda x: x.quantile(0.75)])  # 단일 컬럼에 복수 함수+ 람다식 (리스트)\n",
    "df.groupby('area')['col1'].agg([lambda x:x.mean()-2*x.std(),lambda x:x.mean()+2*x.std()]) # 단일 컬럼 + 복수 람다식\n",
    "                                                                           # .agg([lambda x: , lambda x: ]) 형태에 유의!!!\n",
    "df.groupby('area')[['col1','col2']].agg({'col1':'max','col2':'min'})       # 복수 컬럼에 컬럼별로 다른 함수 적용 (딕셔너리)\n",
    "df.groupby('area')[['col1','col2']].agg(['max','min'])                     # 복수 컬럼에 동일한 복수 함수 (리스트)\n",
    "\n",
    "\n",
    "# .transform() 사용례 (group 별 통계값으로 결측치 대체) 매우 중요!!!!!!!\n",
    "# .apply 사용시 반환되는 index는 원래 컬럼의 index가 아닌 groupby된 상태의 index로 반환되므로\n",
    "# 이 결과를 원래 컬럼에 넣으면 데이터가 엉켜 버린다.\n",
    "# .transform()을 사용시, 원래의 컬럼과 동일한 인덱스로 값을 반환해 준다.\n",
    "df['age'] = df.groupby('sex')['age'].transform(lambda x:  x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태 변환 (pivot_table() | crosstab() | .explode() | melt() ) ---------------\n",
    "\n",
    "# .explode() : 데프|시리즈 안에 있는 리스트 형태의 데이터를 분리해서 행으로 변환 (long-form 변환)\n",
    "df.explode('items') \n",
    "\n",
    "# pivot_table() : wide form 변환시 필수 + 복수 함수 적용시 유용 (함수 + 결측치 처리 + 중복 값 처리 등)\n",
    "# index=인덱스, columns=컬럼 유일값 컬럼 생성, values=집계값, aggfunc=함수, fill_value=결측치값, margins=T|F 합계\n",
    "# columns 내용과 집계값 동일 > columns= 만 지정 | columns 내용과 집계값 다름 > columns=, values= 별도 지정\n",
    "# 'mean'(디폴트),'size'(단순 빈도), 'sum','min','max','median','count'(결측치 X),'std','var','quantile', 사용자 정의 함수 등\n",
    "# 컬럼값을 하나만 넣을 때 중괄호 빼고 입력해야 멀티인덱스가 아닌 단일 인덱스로 생성되니 주의!!!\n",
    "df1 = df.pivot_table(index=['A','B'])  # 다른 값을 지정하지 않으면, 기본적으로 전체 컬럼의 평균값 반환\n",
    "df1 = df.pivot_table(index='A',values=['D','E'],aggfunc={'D':'sum','E':['min','max']})\n",
    "df1 = df.pivot_table(index='Sport', columns='Gender', aggfunc='size')      # 운동종류/성별에 따른 숫자 테이블 반환\n",
    "\n",
    "# pivot() : wide form 변환시 + value 값 그대로 사용시 유용 (함수 적용 불가, 값 중복시 에러 발생)\n",
    "df1 = df.pivot(index='cut', columns = 'variable', values='value').reset_index()\n",
    "\n",
    "# pd.crosstab() : 빈도, 상대도수, 혼동행렬, 상대도수 구할 때 필수! (pivot_table은 빈도만 가능)\n",
    "df1 = pd.crosstab(df['index'], df['col'], margins=True).reset_index()       # 빈도 반환 (margins= 합계 여부)\n",
    "df1 = pd.crosstab(df['index'], df['col'], normalize = 'all').reset_index()  # 비율 반환 ('all', 0 , 1)\n",
    "\n",
    "# melt() : long-form 변환\n",
    "# id_vars = 고정 컬럼(첫번째 인덱스), value_vars = value로 값을 넘길 컬럼(두번째 인덱스)\n",
    "# value 컬럼은 자동 생성됨\n",
    "df1 = df.melt('cut').reset_index() \n",
    "\n",
    "\tName\t수학\t영어\t국어\n",
    "0\t김딴짓\t90\t    92\t    91\n",
    "1\t박분기\t93\t    84\t    94\n",
    "2\t이퇴근\t85\t    86\t    83\n",
    "\n",
    "df.melt(id_vars = ['Name'], value_vars = ['수학', '영어'])\n",
    "    Name\tvariable\tvalue\n",
    "0\t김딴짓\t수학\t    90\n",
    "1\t박분기\t수학     \t93\n",
    "2\t이퇴근\t수학\t    85\n",
    "3\t김딴짓\t영어    \t92\n",
    "4\t박분기\t영어    \t84\n",
    "5\t이퇴근\t영어    \t86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array, series, 데프에서 값 추출 -------------------------------------------------\n",
    "# 시리즈 > series1.iloc[] , series1.values[], series.index[]로 값|인덱스 추출 \n",
    "\n",
    "# 행과 열을 하나만 선택시 반환값의 형태 (Boolean으로 선택한 결과는 시리즈 반환!!!)\n",
    "df.iloc[,]                          # 값 하나 선택시 값 반환\n",
    "df.loc[,]                           # 값 하나 선택시 값 반환\n",
    "df.loc[df['date'] == '01','temp']   # 값 하나 선택시 시리즈 반환 > .values[0]를 해야 값 반환\n",
    "\n",
    "# 컬럼명이 멀티레벨 인덱스일 때 컬럼을 선택해 반환하는 방법\n",
    "df2['survived']['sum']   # 레벨별로 컬럼 인덱스명을 연속해서 지정\n",
    "\n",
    "# 다양한 반환값\n",
    "series1.values   # 시리즈|데프의 값만 어레이로 반환\n",
    "series1.index    # 시리즈|데프의 인덱스 반환\n",
    "series1.items    # 인덱스&값 반환 (굳이 쓸 필요가 있는지 모르겠음)\n",
    "df.max()         # 시리즈|데프|어레이 최대값 반환\n",
    "max(list1)       # 리스트의 최대값 반환\n",
    "\n",
    "c1[c1!=1].max()         # 데프 c1 에서 1을 제외한 최대값 반환 (아래 방향) (상관관계 분석시 유용)\n",
    "c1[c1!=1].max().max()   # 반환된 시리즈의 최대값 반환\n",
    "\n",
    "df.idxmax()                       # 시리즈|데프 최대값 인덱스 \n",
    "df.iloc[:,1:4].idxmax(axis=1)     # 옆 방향 최대값 컬럼명(라벨) 반환!!! 매우 중요!!!!\n",
    "array1.argmax()                   # 어레이 최대값 인덱스 반환 \n",
    "list1.index(max(list1))           # 리스트 최대값 인덱스 반환\n",
    "\n",
    "df['age'].nlargest(1)             # 시리즈 내림차순 인덱스&값 시리즈 반환 (1 입력시 최대값만 반환)\n",
    "df['age'].nlargest(3).values[1]   # 시리즈 내림차순 인덱스&값 시리즈 반환 (3개 선택 후, 2번째 값 반환)\n",
    "df.nlargest(columns='age',n=5)    # 데프   내림차순 인덱스&값 데프 반환 (데프는 columns=,n= 지정 필수)\n",
    "df.nsmallest(columns='age',n=5)   # 데프   올림차순 인덱스&값 데프 반환 (데프는 columns=,n= 지정 필수)\n",
    "df.sort_values('age', ascending=True).iloc[:5,:] # 위와 동일 결과 리턴\n",
    "\n",
    "df['label'].value_counts()                 # 고유값의 빈도 반환 (군집별 갯수 등) (default가 내림차순!!!)\n",
    "df['label'].value_counts(normalize=True)   # 고유값의 비율 반환 (그룹 후 군집별 비율 값)\n",
    "df[['month','day']].value_counts()         # 데프도 가능!!! 중요!!!\n",
    "\n",
    "r1 = pd.Series(model.feature_importances_, index = X_tr.columns) # 컬럼명을 인덱스로하는 시리즈 생성 (중요)\n",
    "\n",
    "(df['replies']==0).sum()        # (Boolean).sum()은 조건을 만족하는 행의 수 반환 \n",
    "df.loc[df['replies']==0].sum()  # 조건을 만족하는 행의 컬럼별 합계 값 반환(주의!!!)\n",
    "\n",
    "df.mean()                  # 평균값 (시리즈|데프)\n",
    "np.mean(array1|list1)      # 평균값 (array|list)\n",
    "\n",
    "value1.round(2)            # 반올림 (numpy.float 경우)\n",
    "round(value1, 2)           # 반올림 (그냥 float  경우)\n",
    "np.floor(value1*100)/100   # 소수점 이하 버림  | int(value1*100)/100 도 동일\n",
    "np.ceil(value1*100)/100    # 소수점 이하 올림 (0.742 -> 0.75)\n",
    "\n",
    "pi//1  # float 값 정수 부분(3) 반환 (pi = 3.14의 경우)\n",
    "pi%1   # float 값 소수점(0.14) 이하 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스칼라 연산 (리스트는 스칼라 연산 불가 > 시리즈로 변환 필요) ----------------------------\n",
    "# axis= 지정을 괄호 안에 하는지 괄호 밖에 하는지 차이가 있으니 주의 깊게 보자!!!\n",
    "# lambda x에서 x는 원본 데이터 보다 한단계 낮은 레벨 : 데프 -> 시리즈, 시리즈 -> 요소\n",
    "\n",
    "df1 = df.applymap(lambda x:x**2)          # 데프만 가능, 전체에 동일 함수 적용시 | df1 = df**2 과 동일\n",
    "df1 = df.apply(lambda x:x.max(), axis=1)  # 시리즈|데프, 각 열/행별 함수 적용시  | df1 = df.max(axis=1) 동일\n",
    "se1 = series1.map(dict1)                  # 시리즈만 가능, 딕셔너리 mapping 기능이 차별화 포인트!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 통계 라이브러리 --------------------------------------------------------------------\n",
    "\n",
    "# VIF만 statsmodels 이고, 나머지는 모두 scipy 라이브러리\n",
    "# 절편을 고려해야 하는 경우 X_tr['const'] = 1 잊지 말자!!!\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif  # VIF 계산 [★★★★★]\n",
    "\n",
    "# scipy 라이브러리\n",
    "from scipy.spatial.distance import pdist   # KMean 중심점 간 거리 계산\n",
    "pdist(model.cluster_centers_)              # 중심점 간 거리 (metric='euclidean'|'cityblock'(맨하탄 거리))\n",
    "\n",
    "from scipy.stats import skew, kurtosis     # 왜도, 첨도 (계산식과 결과 동일)\n",
    "skew(df)                                   # df.skew()는 표본 기준으로 값이 약간 다름\n",
    "kurtosis(df)                               # df.kurt()는 표본 기준으로 값이 약간 다름\n",
    "\n",
    "from scipy.stats import norm     # z-값은 왼쪽부터 누적계산하는 One-tailed probability 방식임\n",
    "nd = norm(mean1, std1)           # 정규 분포 객체, 입력값 생략시 norm(0,1)이 default\n",
    "ppf_val = nd.ppf(0.5+0.95/2)     # 신뢰구간에 해당하는 z값 계산 (입력시 0.5+신뢰구간/2 로 입력)\n",
    "cdf_val = nd.cdf(z_value)        # z값을 넣으면 누적 확률 값 계산, ppf와 역의 관계 (1.96 입력->0.975 출력)\n",
    "pdf_val = nd.pdf(z_value)        # z의 확률 값\n",
    "\n",
    "from scipy.stats import poisson           \n",
    "prob = poisson.pmf(x, lambda)    # Poisson 분포 확률 값, (x: 확률을 계산할 값, lambda : 람다)\n",
    "\n",
    "from scipy.stats import binom              \n",
    "binom.pmf(성공횟수, 시행횟수, 1회 시행시 성공확률)  # 이항 분포 확률 값\n",
    "\n",
    "from scipy.stats import t           \n",
    "prob = t.pdf(x, dof)             # t 분포 확률 값, (x: 확률을 계산할 값, dof : 자유도)\n",
    "\n",
    "from scipy.stats import gmean    # 기하평균\n",
    "gmean(data)                      # 데이터는 리스트 형태로 입력\n",
    "\n",
    "from scipy.stats import hmean    # 조화평균\n",
    "hmean(data)                      # data는 리스트 형태로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### 중요 구문 정리 #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(), range() 비교\n",
    "# np.arange() : 바로 결과 반환 가능       | 실수 사용 가능 \n",
    "# range()     : 리스트 컴프리헨션 등 필요 | 실수 사용 불가\n",
    "# 만약 0.05부터 0.95까지 리스트를 만들고자 한다면\n",
    "np.arange(0.05, 1.0, 0.05).tolist()\n",
    "[i/100 for i in range(5, 100, 5)]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop 연산시 문자열로 변수명 생성\n",
    "# 리스트에 포함된 문자열을 이용해서 LTE, UMTS, GSM 데프 생성 사례\n",
    "list1 = ['LTE','UMTS', 'GSM']\n",
    "for col in list1:\n",
    "    locals()[col] = df.loc[df['TECH']== col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy는 왼쪽부터 누적계산하는 One-tailed probability방식 -> 신뢰구간 95%면 0.5+0.95/2=0.975 입력\n",
    "\n",
    "# 신뢰수준 95%의 신뢰구간 상한,하한 계산\n",
    "from scipy.stats import norm, t\n",
    "c_level = 0.95                         # 신뢰수준\n",
    "mean    = df['salary'].mean()          # 평균\n",
    "std_dev = df['salary'].std()           # 표준편차\n",
    "n       = len(df)                      # 샘플 갯수 (데프 행 수)\n",
    "\n",
    "# 1. 정규분포 : norm.ppf()\n",
    "z1 = norm.ppf(0.5+c_level/2)           # 정규분포의 Z값 계산(95%일 때 약 1.96 !!!)\n",
    "se = z1 * std_dev/(n**0.5)             # 표준오차(standard error) 계산\n",
    "lb = mean - se                         # 신뢰구간 상한값\n",
    "ub = mean + se                         # 신뢰구간 하한값\n",
    "\n",
    "# 1-1. t분포 : t.ppf()\n",
    "t1 = t.ppf((0.5+c_level/2), 자유도)    # t분포의 t값 계산 (신뢰수준, 자유도(n-1)) 입력\n",
    "se = t1 * std_dev/(n**0.5)             # 표준오차(standard error) 계산\n",
    "lb = mean - se                         # 신뢰구간 상한값\n",
    "ub = mean + se                         # 신뢰구간 하한값\n",
    "\n",
    "# 2. .interval() 이용\n",
    "confidence_interval = norm.interval(c_level, mean, std_dev/(n**0.5))\n",
    "lower_bound = confidence_interval[0]   # 신뢰구간 상한값\n",
    "upper_bound = confidence_interval[1]   # 신뢰구간 하한값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거 방법 ------------------------------------------------------------\n",
    "# 구간내 데이터를 선택하는 방법과, 이상치에 None입력 후 dropna로 제거하는 방법이 있음\n",
    "\n",
    "# 이상치를 하나의 기준으로 선택\n",
    "- 1개 컬럼 : df1 = df.loc[(lb <= df['col']) & (df['col']<= ub)]            # 구간 내 데이터 선택\n",
    "- 1개 컬럼 : df.loc[(df['col'] < lb) | (ub < df['col']), 'col'] =  None    # 이상치에 None 입력 후 dropna로 제거\n",
    "- 1개 컬럼 : df1 = df['col'].drop(index = outliers.index)                  # 이상치 index를 이용해 drop으로 제거\n",
    "- 복수개 컬럼 : 이상치에 None 입력 후 dropna로 제거\n",
    "\n",
    "\n",
    "# 이상치 제거\n",
    "q1, m1, q3   = df1['price'].quantile([0.25, 0.5, 0.75])   # 복수개 값 반환 가능\n",
    "iqr  = q3 - q1\n",
    "lb   = m1 - 2.3*iqr\n",
    "ub   = m1 + 2.3*iqr\n",
    "df11 = df1.loc[(lb <= df1['price'])&(df1['price'] <= ub)]        \n",
    "\n",
    "\n",
    "# 컬럼 순회하며, 각 행의 이상치에 None 입력 -> 결측치가 있는 행 일괄 제거 \n",
    "for col in cols:\n",
    "   q1, q3   = df1['price'].quantile([0.25, 0.75])  \n",
    "   iqr = q3 - q1\n",
    "   lb  = q1 - 1.5*iqr\n",
    "   ub  = q3 + 1.5*iqr\n",
    "   df.loc[(df[col] < lb) | (ub < df[col]), col] =  None   # 중요!!!!!\n",
    "df1 = df.dropna()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 그룹별로 이상치 판별식에 따라 이상치를 판별하고 제거\n",
    "\n",
    "# labmda를 이용해서 한번에 구하는 경우\n",
    "dfb = df.groupby('근속년수')['연봉'].agg([lambda x:x.mean() - 2*x.std(), lambda x:x.mean() + 2*x.std()]) # lb, ub 반환 (rename 필요)\n",
    "dfm = pd.merge(df, dfb)                                            # 원 데이터에 그룹별 lb, ub 컬럼 merge\n",
    "df1 = dfm.loc[(dfm['lb']<=dfm['연봉'])&(dfm['연봉']<=dfm['ub'])]   # 구간 내 데이터 선택\n",
    "\n",
    "# 2 단계로 나누어 구하는 경우\n",
    "dfb = df.groupby('근속년수')['연봉'].agg(['mean', 'std']) # 이상치 판별에 사용할 통계값 컬럼 생성\n",
    "dfb['lb'] = dfb['mean'] - 2*dfb['std']                    # 이상치 판별에 사용할 ub, lb 생성\n",
    "dfb['ub'] = dfb['mean'] + 2*dfb['std']\n",
    "dfm = pd.merge(df, dfb)    # 원 데이터에 이상치 판별용 데이터 merge (원 데프의 공통 컬럼에 맞춰 우측 데프가 복사 됨 (중요!!!))\n",
    "df1 = dfm.loc[(dfm['lb'] <= dfm['연봉'])&(dfm['연봉']<= dfm['ub'])] # 이상치 판별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .groupby()를 이용해 결측치 또는 특정값을 그룹별 통계값으로 대체 \n",
    "# .apply 사용시 반환되는 index는 원래 'age'의 index가 아닌 groupby된 상태의 multi-index형태로 반환이 되므로\n",
    "# 이 결과를 df['age'] 컬럼에 넣었다가는 데이터가 엉켜 버린다.\n",
    "df.groupby('sex')['age'].apply(lambda x : x.fillna(x.mean()))\n",
    "\n",
    "# .transform()을 사용하면, 원래의 'age'컬럼과 동일한 인덱스로 값을 반환해 준다.\n",
    "df['age'] = df.groupby('sex')['age'].transform(lambda x:  x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "# 1. .transform() 이용  [★★★★★]\n",
    "#.transform() -> 원래 컬럼의 인덱스와 동일한 형태로 값을 반환\n",
    "#.apply() -> 원래 컬럼의 index가 아닌 groupby된 상태의 multi-index로 반환되므로 사용 불가\n",
    "\n",
    "# 0값을 평균으로 대체하는 경우 (평균을 구할 때 0도 포함하여 계산하는 경우)\n",
    "df['SET'] = df.groupby('TECH')['SET'].transform(lambda x: x.replace(0, x.mean())) # 결측치를 그룹별 평균값으로 채움\n",
    "\n",
    "# 결측치를 그룹별 평균값으로 대체하는 경우\n",
    "# groupby 괄호 안 컬럼에 직접 계산시 'age'//10*10이 아니라 df['age']//10*10 형태로 입력!!!\n",
    "df['bmi'] = df.groupby(df['age']//10*10)['bmi'].transform(lambda x: x.fillna(x.mean())) # 결측치를 동일 나이대 평균값으로 대체\n",
    "df['age'] = df.groupby('sex')['age'].transform(lambda x:  x.fillna(x.mean())) # 결측치를 동일 성별의 평균값으로 대체\n",
    "\n",
    "# 특정값(0 또는 특정 범위의 값)을 그룹별 평균값으로 대체하는 경우\n",
    "df['SET'] = df['SET'].replace(0, np.NaN) # 해당값을 결측치로 변환\n",
    "df.loc[df['SET'] <= 0, 'SET'] = np.NaN   # 해당값을 결측치로 변환\n",
    "df['SET'] = df.groupby('TECH')['SET'].transform(lambda x: x.fillna(x.mean())) # 결측치를 그룹별 평균값으로 채움\n",
    "\n",
    "# 2. 딕셔너리와 .map을 이용 (.transform() 을 이용하면 이 방법을 굳이 쓸 필요가 없음!!!)\n",
    "# {연령대 그룹:그룹별 평균값} dictionary 생성\n",
    "dict1 = df.groupby(df['age']//10*10)['bmi'].mean().to_dict() \n",
    "# 결측치행 'bmi' 선택 > 선택 행 연령대 계산 -> {연령대 그룹:그룹별 평균값} 으로 매핑 \n",
    "# [  ,'age'//10*10] 아니라, [  , 'age']//10*10에 주의!!!, (전체 괄호).map() 주의!!!)\n",
    "df.loc[df['bmi'].isna(),'bmi'] = (df.loc[df['bmi'].isna(),'age']//10*10).map(dict1)  # 딕셔너리로 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 결측치 처리 방법 -----------------------------------------------------------\n",
    "# 1. 특정값으로 일괄 처리\n",
    "- 1개 값 입력 : df = df.fillna(value1)                          # 데프 전체\n",
    "- 대표값 입력 : df['col'] = df['col'].fillna(df['col'].mean())  # 컬럼 1개\n",
    "- 컬럼별 대표값 입력 : for loop 이용\n",
    "\n",
    "for col in cols:                                                 \n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "- categorical 데이터 : df1['deck'].dtype  # CategoricalDtype 확인\n",
    "                       df1['deck'] = df1['deck'].cat.add_categories('No Data').fillna('No Data') \n",
    "\n",
    "# 2. 결측치 컬럼의 group별 통계값으로 처리 (.transform()을 이용해 간단하게 해결)\n",
    ".apply() 이용시 원래 컬럼의 index가 아닌 groupby된 상태의 multi-index로 반환되므로 쓸 수 없음\n",
    "\n",
    ".transform() 메소드를 사용하면, 원래 컬럼의 인덱스로 재정렬해서 값을 반환\n",
    "\n",
    "df1['bmi'] = df1.groupby(df['age']//10*10)['bmi'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df4['age'] = df4.groupby('sex')['age'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "# 2-1. 결측치 컬럼의 group별 통계값으로 처리 (딕셔너리와 .map을 이용한 방법인데 굳이 쓸 필요 없음.)\n",
    "\n",
    "dict1 = df.groupby(df['age']//10*10)['bmi'].mean().to_dict()           # {그룹값:대표값} 딕셔너리 생성\n",
    "\n",
    "df.loc[df['bmi'].isna(),'bmi'] = (df.loc[df['bmi'].isna(),'age']//10*10).map(dict1)  # .map()으로 처리\n",
    "\n",
    "# 3. 결측치를 개별값으로 처리\n",
    "- 앞/뒤 값  : df.ffill(), df.bfill()                 # ffill : 앞의 값으로 뒤의 결측치 처리\n",
    "- 1개 컬럼  : df['col'].ffill(), df['col'].bfill()   # bfill : 뒤의 값으로 앞의 결측치 처리\n",
    "- 보간법    : 결측치의 인덱스 + 결측치 앞뒤 값과의 관계식 + for loop\n",
    "- 머신러닝  : 결측치 포함않은 행을 tr, 포함 행을 te로 분리 -> te에서 결측치 포함되지 않은 컬럼을 X_te로 생성\n",
    "              -> X_te를 이용해 예측한 y_pr로 결측치를 처리\n",
    "                 tr |  X_tr |  y_tr  | -> 훈련\n",
    "                 te |  X_te | 결측치 | -> X_te로 y_pr(결측치)을 예측하고 결측치를 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 숫자의 배수인 행과, 배수가 아닌 행으로 분리\n",
    "\n",
    "te = df.loc[df['col1']%4==0]     # 4의 배수 행 선택\n",
    "tr = df.loc[df['col1']%4!=0]     # 4의 배수 아닌 행 선택 \n",
    "tr = df.drop(te.index)           # 여집합을 활용한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 평가 결과를 리스트로 반환 > n번째 큰 k값 구하기\n",
    "\n",
    "l1 = []\n",
    "for n in range(2,10):\n",
    "    model = KMeans(n_clusters=n, random_state=123).fit(p1)\n",
    "    ss = silhouette_score(p1, model.labels_)\n",
    "    l1 += [(ss, n)]\n",
    "sorted(l1, reverse=True)[0]   # 최대값일 경우 0 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr, te 분리 후 스케일시, tr로 fit()한 객체 이용해 tr, te 모두 transform()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sc = StandardScaler().fit(tr)   # fit()\n",
    "trs = sc.transform(tr)          # transform()\n",
    "tes = sc.transform(te)          # transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정 컬럼별 순회하며 컬럼의 값을 기준으로 순위값 컬럼 추가\n",
    "for col in ['GRE', 'TOEFL']:                                    \n",
    "    df[col+'_R'] = df[col].rank(ascending=False, method='min')  # 랭크 컬럼 생성('min':1,2,2,4 | 'max':1,3,3,4)\n",
    "\n",
    "# 기준 컬럼없이 그냥 나열된 순서대로 rank 매기는 방법 (중복처리 없이 일괄적 번호 매김)\n",
    "# .groupby().cumcount() 각 그룹의 각 행의 순서 값을 반환 (groupby() 전용 메서드 임!!!)\n",
    "df['rank'] = df.groupby('year').cumcount() + 1   # 'year'별 순서값 반환, 시작값이 0에 주의!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby().apply()로 정렬 -> cumsum() -> (max-mix) 계산\n",
    "# 중요한 것은 한번에 처리하기 어려운 경우, 단계를 나누어서 차근 차근 진행하면 가능하다는 것!!! \n",
    "\n",
    "p11 = p1.groupby('area').apply(lambda x: x.sort_values('date')).reset_index(drop=True) # area별 date로 sort된 데프 생성, reset_index 중요\n",
    "p11['cum_rain'] = p11.groupby('area')['rain'].cumsum()                                 # area별 rain의 cumsum을 저장\n",
    "r1 = p11.groupby('area')['cum_rain'].apply(lambda x : x.max()-x.min())\n",
    "r1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정 컬럼에서 가장 큰 n개의 값을, 그 중 최소값으로 대체 예제\n",
    "\n",
    "# 1. .nlargest() 이용한 방법 (index를 그대로 유지하면서 변환 가능)\n",
    "#.nlargest() 의 결과는 새롭게 반환된 데프를 참조하므로, 값 입력시 원본 데프 변경 안 됨\n",
    "# -> .nlargest() 결괏값의 index를 활용해서 df.loc[] 형태로 값을 입력해 주어야 함 !!!\n",
    "l1 = df.nlargest(n=10, columns='CRIM').index.tolist()     # n=, columns= 필수 적용 (갯수만 지정시는 숫자만 입력해도 됨)\n",
    "min_val = df.nlargest(n=10, columns='CRIM')['CRIM'].min() # 최소값 반환\n",
    "df.loc[l1, 'CRIM'] = min_val                              # 최소값으로 원본 데프 값 대체\n",
    "\n",
    "# 2. sort_values() 이용한 방법 (reset_index 필요)\n",
    "l1 = df.sort_values('CRIM',ascending=False).index.tolist()            # 10개 상위값의 index를 리스트로 저장\n",
    "min_val = df.sort_values('CRIM', ascending=False)[:10]['CRIM'].min()  # 10개중 최소값 반환\n",
    "df.loc[l1, 'CRIM'] = min_val      # 10개의 상위값을 10개중 최소값으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트의 요소가 많을 때, 어떤 조건을 만족하는 요소만 반환하는 list comprehension\n",
    "# 예를들어, unique값이 많은데 'Hz'라는 문자열이 있는 요소만 찾고 싶다!!!\n",
    "\n",
    "pd.Series(p11['Frequency'].unique())[pd.Series(p11['Frequency'].unique()).str.contains('Hz')]\n",
    "# 앞 부분에서 unique()를 시리즈로 반환, 뒷 부분은 불리언 값을 [[  ]] 형태로 반환\n",
    "# 시리즈에 시리즈[[불리언]] 형태로 적용하면, 시리즈에서 불리언이 True 인 값만 반환하게 됨.\n",
    "# 시리즈는 컬럼이 하나 밖에 없기 때문에 iloc나 loc 없이 바로 인덱싱을 적용할 수 있음\n",
    "\n",
    "# 리스트에서 특정 문자열 포함 요소 반환 : (x) for (x) in (리스트) if (문자열) in (x) 구문\n",
    "l1 = p11['Frequency'].unique().tolist()    # 컬럼의 unique 값 리스트 생성\n",
    "[x for x in l1 if 'Hz' in x]               # for in if in, 리스트에서 'Hz' 포함하는 요소 반환 (구문 암기!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데프 복수의 컬럼에 흩어져 있는 특정 문자열('60 Hz')을 포함하는 행의 갯수 확인\n",
    "'''만약 컬럼별로 '60 Hz'가 포함된 갯수를 구한 후 더하는 경우 한 행에 중복으로 들어간 데이터가 있다면 틀리게 됨\n",
    "   따라서 행별로 '60 Hz'가 포함된 컬럼의 수를 카운트 한 후, 카운트가 1 이상인 행의 갯수를 구해야 함'''\n",
    "\n",
    "p1['cnt'] = p1.apply(lambda x : x.str.contains('60 Hz').sum(), axis=1)   # 행별로 '60 Hz' 포함한 컬럼 갯수를 구해 컬럼 생성\n",
    "p1.loc[p1['cnt'] >= 1]      # 1 이상인 행만 확인 (중복의 경우가 있을 수 있으므로 ==1 이 아닌  >=1 로 한다.)\n",
    "(p1['cnt'] >= 1).sum()      # 행의 갯수 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 pd. 메소드 ----------------------------------------------------------------------\n",
    "# pandas는 객체지향(df.melt)과 함수형(pd.melt())을 모두 지원하지는 않음 > 함수형만 별도로 외우자!!!\n",
    "\n",
    "pd.cut(df['col'], n)  # 동일한 구간으로 나누기\n",
    "pd.qcut(df['col'], n) # 동일한 샘플 갯수로 나누기\n",
    "pd.read_csv('bike.csv')\n",
    "pd.concat([df1, df2], axis=1)\n",
    "pd.merge(df1, df2, left_on='co1', right_on='co2', how='left')           \n",
    "pd.crosstab(df1['row'], df1['col'], margins=True)        # 반환값:카운트 (margins= 합계 여부)\n",
    "pd.get_dummies(df, columns =['color'], drop_first=True)  # 원 데이터프레임에 자동 병합                   \n",
    "pd.to_datetime(df['Date'], errors = 'coerce') # errors = 'coerce' : 잘못된 문자열은 NaN으로 반환\n",
    "pd.to_numeric(df['col'], errors = 'coerce')   # errors = 'coerce' : 잘못된 문자열은 NaN으로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 파라미터 확인 ------------------------------------------------------------------\n",
    "# 많이 쓰는 구문인데 error 났을 때 바로 뭔지 모르겠다면 ? 또는 help()로 빨리 확인!!!\n",
    "pd.concat?\n",
    "help(pd.concat)          # pd.DataFrame.rank | np.mean | LinearRegression 등\n",
    "help(str.join)\n",
    "\n",
    "# 패키지 메소드 확인 ------------------------------------------------------------------\n",
    "dir(pd.DataFrame)              # pd.Series, pd, np | sklearn | scipy | statsmodels 등\n",
    "from statsmodels.stats.ou      # 쥬피터 노트북에서는 탭을 누르면 다음 힌트를 볼 수 있다.\n",
    "\n",
    "# 특정 메소드가 패키지/라이브러리에 포함되어 있는지 확인 ------------------------------\n",
    "dir(pd).count('to_datetime')   # 결과가 1이면, '.to_datetime'은 pd의 메소드!!!\n",
    "dir(np).count('where')         # 결과가 1이면 '.where'는 np의 메소드!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레벨3 에서 공부 할 것 ----------------------------------------------------------------\n",
    "\n",
    "# 화면에 표시되는 데이터프레임의 행, 컬럼 수 옵션 설정\n",
    "pd.set_option('display.max_columns', None) # 모든 열 출력\n",
    "pd.set_option('display.max_rows', None)    # 모든 행 출력\n",
    "\n",
    "pd.options.display.max_columns = None    # 전체 컬럼 표시 \n",
    "pd.options.display.max_rows = None       # 전체 행 표시\n",
    "pd.options.display.max_colwidth = None   # 한 컬럼의 너비를 무제한으로 표시\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau # 상관계수, p-value 반환 (입력 형태 주의)\n",
    "from scipy.stats import skew, kurtosis\n",
    "c1, p1 = pearsonr(df['col1'], df['col2'])   # 한 쌍만 계산 가능!!!\n",
    "s1     = skew(데이터)                       # 1차원 array, 데이터프레임만 입력 가능\n",
    "k1     = kurtosis(데이터)                   # scipy의 kurtosis는 모집단 기준 계산 (결괏값 pandas와 차이)\n",
    "\n",
    "# 10단위가 아닌 연령대 변환 > pd.cut() 이용\n",
    "ages   = pd.Series(age_list)                  # 1.시리즈로 변환 (반드시 시리즈로!!!)\n",
    "b1     = [0, 18, 35, 55, np.inf]              # 2.bins 리스트 생성\n",
    "l1     = ['<18', '18-34', '35-54', '55+']     # 3.labels 리스트 생성 (반드시 bins-1개)\n",
    "ages_b = pd.cut(ages, bins=b1, labels=l1)     # 4.범주형 변수로 변환\n",
    "\n",
    "pd.cut(df['col'], n)                            # 동일한 구간으로 나누기\n",
    "pd.qcut(df['col'], n, labels=['gr1','gr2',...]) # 동일한 샘플 갯수로 나누기 (labels= 생략 가능)\n",
    "\n",
    "# 숫자인지 아닌지를 판별하는 방법\n",
    "df.loc[~df['age'].str.isdigit(), 'age']          # 숫자가 아닌 데이터가 있을 경우 반환 (소수점 없는 경우만 가능)\n",
    "df.loc[df['temp'].str.contains(r'^\\D'), 'temp']  # 소수점 포함된 경우 > 정규식으로 (r'^\\D') 문자로 시작하는 경우 반환\n",
    "\n",
    "df.columns[df.eq('09-30').any(axis=1)]        # 조건 값 하나 > 컬럼명 반환\n",
    "\n",
    "df.resample('W').agg(['mean','std'])          # 시계열 데이터를 재샘플링, ('W' : 주 단위로 데이터그룹화)\n",
    "\n",
    "display(df1)   # 복수개 데이터 확인시 유용 (데프 그대로 보여줌)\n",
    "display(df2)\n",
    "\n",
    "df.insert(0, 'sum', df.sum(axis=1))                # 원하는 위치에 컬럼 추가 (인덱스값, 컬럼명, 값)\n",
    "df.insert(2,'family', (df['sibsp'] + df['parch'])) # 컬럼값 연산값을 입력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
