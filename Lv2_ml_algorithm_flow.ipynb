{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도 학습 | sklearn 라이브러리 이용 =================================================\n",
    "# 분석 순서 : tr/te 분할  ->  y_tr/X_tr, y_te/X_te 분할  ->  X_tr, X_te 정규화  ->  fit()\n",
    "# loop 내에서 독립변수 변경시  : tr/te 분할(loop 밖) > y_tr/X_tr, y_te/X_te 분리 (loop 내) -> fit() \n",
    "\n",
    "\n",
    "# train, test 데이터 분리 ----------------------------------------------------------\n",
    "# sklearn은 fit_intercept=True가 default라 상수항 추가 불필요\n",
    "# stratify='col1' 지정시 해당 컬럼의 비율을 유지하도록 층화 추출\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr, te = train_test_split(df, train_size=0.8, random_state=123)\n",
    "\n",
    "y_tr = tr['col1']\n",
    "X_tr = tr[['col2','col3']]   # X는 컬럼이 하나라도 반드시 2차원 데프 형태로 지정!!!\n",
    "y_te = te['col1']\n",
    "X_te = te[['col2','col3']]   # X는 컬럼이 하나라도 반드시 2차원 데프 형태로 지정!!!\n",
    "\n",
    "\n",
    "\n",
    "# 정규화/표준화 ---------------------------------------------------------------------\n",
    "# fitting한 instance 재사용 필요시 > scaler 인스턴스 생성 필요  [★★★★★]\n",
    "# tr/te 분리 후 표준화시, tr로 fit() 후에 tr과 te를 transform() 해야 함!!!\n",
    "from sklearn.preprocessing  import MinMaxScaler, StandardScaler\n",
    "\n",
    "sc    = MinMaxScaler().fit(X_tr)\n",
    "X_trs = pd.DataFrame(sc.transform(X_tr), columns= X_tr.columns)   # 어레이 -> 데프로 반환\n",
    "X_tes = pd.DataFrame(sc.transform(X_te), columns= X_tr.columns)   # 어레이 -> 데프로 반환\n",
    "\n",
    "\n",
    "# fitting을 1번만 진행할 경우 > scaler 인스턴스 생성 불필요\n",
    "sc = MinMaxScaler().fit_transform(df)      # 어레이 반환\n",
    "sc = StandardScaler().fit_transform(df)    # 어레이 반환\n",
    "df[['col1','col2']] = sc                   # 어레이를 기존 데프 컬럼에 입력 가능\n",
    "\n",
    "\n",
    "\n",
    "# VIF 구하기 [★★★★★] ----------------------------------------------------------------\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "X_tr['const'] = 1            # 상수항 추가 조건이 있을 경우 필수!!! (statsmodels 함수라...)\n",
    "vifs   = [[vif(X_tr, i), X_tr.columns[i]] for i in range(X_tr.shape[1])] # [VIF, 컬럼명] 리스트로 반환\n",
    "vif_df = pd.DataFrame(vifs, columns = ['VIF', 'VARIABLE'])               # vifs 리스트를 데프로 변환\n",
    "sorted(vifs, reverse=True)[0]                                            # VIF가 최대인 값과 컬럼명 반환\n",
    "\n",
    "\n",
    "# model 생성, 학습 (문제에서 주어지는 입력 옵션 반드시 확인!!!) ---------------------\n",
    "from sklearn.linear_model    import LinearRegression,      LogisticRegression\n",
    "from sklearn.tree            import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble        import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors       import KNeighborsRegressor,   KNeighborsClassifier\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "\n",
    "model =       LinearRegression().fit(X_tr, y_tr)  # 입력값 순서(X,y)에 주의\n",
    "model =     LogisticRegression().fit(X_tr, y_tr)\n",
    "model =    KNeighborsRegressor().fit(X_tr, y_tr)  # n_neighbors=\n",
    "model =   KNeighborsClassifier().fit(X_tr, y_tr)  # 맨해튼 거리 : p=1, 거리 역순 : weights='distance'\n",
    "model =  DecisionTreeRegressor().fit(X_tr, y_tr)  # max_depth=, random_state=\n",
    "model = DecisionTreeClassifier().fit(X_tr, y_tr)  \n",
    "model =  RandomForestRegressor().fit(X_tr, y_tr)  # n_estimators=트리수, min_samples_leaf=분기멈춤 최소 샘플\n",
    "model = RandomForestClassifier().fit(X_tr, y_tr) \n",
    "model =             GaussianNB().fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "# model 결과 속성값 확인 --------------------------------------------------------------\n",
    "model.coef_                # 기울기 (regressor)\n",
    "odds = np.exp(model.coef_) # odds비 계산\n",
    "\n",
    "model.intercept_           # 절편 (regressor)\n",
    "model.feature_importances_ # feature 중요도 (classifier)\n",
    "\n",
    "\n",
    "# predict -----------------------------------------------------------------------------\n",
    "# 학습 데이터가 표준화된 경우, 회귀값 예측은 반드시 표준화 전 데이터 사용!!! (분류는 관계 없음)\n",
    "y_pr       = model.predict(X_te)        # classifier : 1, 0 값 | regressor : 예측 값\n",
    "y_pr_proba = model.predict_proba(X_te)  # classifier : [0 확률 값, 1 확률 값] 반환\n",
    "\n",
    "\n",
    "# Regressor 성능평가 ------------------------------------------------------------------\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "mse  = ((y_te-y_pr)**2).mean()             # 계산식 결과는 함수값과 반드시 비교!!! (y_te는 시리즈) \n",
    "\n",
    "y_pr = np.array([y_pr]*y_te.shape[0])      # y_pr 이 1개인 경우, 함수 이용시 y_te 행 갯수만큼 복사 필요\n",
    "mse  = mean_squared_error(y_te, y_pr)      \n",
    "rmse = mse**0.5                            # 또는 rmse = np.sqrt(mse)\n",
    "\n",
    "mae  = mean_absolute_error(y_te, y_pr)    \n",
    "mape = mean_absolute_percentage_error(y_te, y_pr)  # *100을 해야 퍼센트값 임 주의!!!\n",
    "\n",
    "r2   = r2_score(y_te, y_pr)                # 참값, 예측값 입력 순서 중요!!!\n",
    "\n",
    "\n",
    "# Classifier 성능평가 -----------------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "accuracy  =  accuracy_score(y_te, y_pr)    # 0~1 | 0~100범위 확인하고 답을 적자!!!\n",
    "roc       =   roc_auc_score(y_te, y_pr)\n",
    "f1        =        f1_score(y_te, y_pr)\n",
    "precision = precision_score(y_te, y_pr)\n",
    "recall    =    recall_score(y_te, y_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비지도 학습 | skearn K-Means 라이브러리 이용 ===========================================\n",
    "\n",
    "\n",
    "# 정규화/표준화 ---------------------------------------------------------------------\n",
    "# fitting을 1번만 진행할 경우 > scaler 인스턴스 생성 불필요\n",
    "from sklearn.preprocessing  import MinMaxScaler, StandardScaler\n",
    "\n",
    "sc = MinMaxScaler().fit_transform(df)      # 어레이 반환\n",
    "sc = StandardScaler().fit_transform(df)    # 어레이 반환\n",
    "df[['col1','col2']] = sc                   # 어레이를 기존 데프 컬럼에 입력 가능\n",
    "\n",
    "\n",
    "# model 생성, 학습 (문제에서 주어지는 입력 옵션 반드시 확인!!!) -------------------------\n",
    "# n_clusters=클러스터 갯수, n_init= 초기 중심점 랜덤 선택 횟수, max_iter=반복횟수, random_state=123\n",
    "# min_samples_leaf= 가지에 남는 최소의 샘플 갯수 [★★★★★]\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3, n_init=50, random_state=123).fit(df)  # 어레이|데프 입력 가능\n",
    "\n",
    "\n",
    "# model 결과 속성값 확인 ----------------------------------------------------------------\n",
    "# 군집 결과를 이용해 추가 계산시 반드시 표준화 전의 데이터 이용!!!!!\n",
    "# 군집 결과를 원본 df에 컬럼으로 추가시 후속 분석 작업에 영향을 미치니 주의!!!\n",
    "pd.Series(model.labels_).value_counts()                 # 군집별 요소 갯수 반환 (시리즈 변환 필수)\n",
    "pd.Series(model.labels_).value_counts(normalize=True)   # 군집별 상대도수 반환  (시리즈 변환 필수)\n",
    "pd.Series(model.labels_).value_counts().max()           # 군집별 요소 갯수 중 최대값 반환\n",
    "\n",
    "\n",
    "# 군집 중심점간의 거리 계산 [★★★★★] -----------------------------------------------------\n",
    "from scipy.spatial.distance import pdist      # pairwise distance (두점 간 거리 계산)\n",
    "\n",
    "model.cluster_centers_             # 중심점 값\n",
    "pdist(model.cluster_centers_)      # 중심점 간 거리 (metric='euclidean'|'cityblock'(맨하탄 거리))\n",
    "\n",
    "\n",
    "# 성능 평가 -----------------------------------------------------------------------------\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples # samples를 평균한 값이 score\n",
    "\n",
    "silhouette_score(df, model.labels_)       # (fit용 데이터, label 값) 입력값, 순서 주의!!!\n",
    "silhouette_samples(df, model.labels_)     # (fit용 데이터, label 값) 입력값, 순서 주의!!!\n",
    "\n",
    "\n",
    "# (실루엣 계수, k값) 리스트 생성 후 n번째 값 반환 [★★★★★]\n",
    "l1 =[]\n",
    "for k in range(2, 10):\n",
    "    model = KMeans(n_clusters=k, random_state = 123).fit(df)\n",
    "    sc = silhouette_score(df, model.labels_)\n",
    "    l1 += [(sc, k)]                # (실루엣 값, k값) 튜플의 리스트 (정렬 기준 값 먼저 입력)\n",
    "r1 = sorted(l1, reverse=True)[0]   # 실루엣 값 기준 내림차순 정렬 후, n번째 값 선택 (최대값은 [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도 학습 | statsmodels 라이브러리 이용 ================================================\n",
    "# sklearn만으로 모든 것이 가능하기 때문에 굳이 statsmodels를 이용할 필요는 없음\n",
    "# sklearn은 fit_intercept=True가 default라 상수항 추가 불필요\n",
    "\n",
    "# train, test 데이터 분리 -------------------------------------------------------------\n",
    "# stratify='col1' 지정시 해당 컬럼의 비율을 유지하도록 층화 추출\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr, te = train_test_split(df, train_size=0.8, random_state=123)\n",
    "\n",
    "y_tr = tr['col1']\n",
    "X_tr = tr[['col2','col3']]   # X는 컬럼이 하나라도 반드시 2차원 데프 형태로 지정!!!\n",
    "y_te = te['col1']\n",
    "X_te = te[['col2','col3']]   # X는 컬럼이 하나라도 반드시 2차원 데프 형태로 지정!!!\n",
    "\n",
    "\n",
    "# VIF 구하기 [★★★★★] ----------------------------------------------------------------\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "X_tr['const'] = 1            # 상수항 추가 조건이 있을 경우 필수!!! (statsmodels 함수라...)\n",
    "vifs   = [[vif(X_tr, i), X_tr.columns[i]] for i in range(X_tr.shape[1])] # [VIF, 컬럼명] 리스트로 반환\n",
    "vif_df = pd.DataFrame(vifs, columns = ['VIF', 'VARIABLE'])               # vifs 리스트를 데프로 변환\n",
    "sorted(vifs, reverse=True)[0]                                            # VIF가 최대인 값과 컬럼명 반환\n",
    "\n",
    "\n",
    "# model 생성, 학습 (문제에서 주어지는 입력 옵션 반드시 확인!!!) ----------------------\n",
    "from statsmodels.api import OLS, Logit\n",
    "\n",
    "model =   OLS(y_tr, X_tr).fit()   # 입력 위치와 입력값 순서(y, X)에 주의\n",
    "model = Logit(y_tr, X_tr).fit()\n",
    "\n",
    "\n",
    "# model 결과 속성값 확인 --------------------------------------------------------------\n",
    "model.params     # 절편, 기울기 값 (regressor)\n",
    "model.rsquared   # R2 (regressor)\n",
    "model.summary()  # R2, Adj-R2, 회귀계수 등 (regressor)\n",
    "\n",
    "\n",
    "# predict -----------------------------------------------------------------------------\n",
    "y_pr = model.predict(X_te)      # classifier : 확률 값 반환 (주의!!), regressor: 예측 값\n",
    "y_pr = (y_pred>0.5).astype(int) # classifier : 확률값을 1,0 값으로 변환\n",
    "\n",
    "\n",
    "# Regressor 성능평가 ------------------------------------------------------------------\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "mse  = ((y_te-y_pr)**2).mean()             # 계산식 결과는 함수값과 반드시 비교!!! (y_te는 시리즈) \n",
    "\n",
    "y_pr = np.array([y_pr]*y_te.shape[0])      # y_pr 이 1개인 경우, 함수 이용시 y_te 행 갯수만큼 복사 필요\n",
    "mse  = mean_squared_error(y_te, y_pr)      \n",
    "rmse = mse**0.5                            # 또는 rmse = np.sqrt(mse)\n",
    "\n",
    "mae  = mean_absolute_error(y_te, y_pr)    \n",
    "mape = mean_absolute_percentage_error(y_te, y_pr)  # *100을 해야 퍼센트값 임 주의!!!\n",
    "\n",
    "r2   = r2_score(y_te, y_pr)                # 참값, 예측값 입력 순서 중요!!!\n",
    "\n",
    "\n",
    "\n",
    "# Classifier 성능평가 -----------------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "accuracy  =  accuracy_score(y_te, y_pr)    # 0~1 | 0~100범위 확인하고 답을 적자!!!\n",
    "roc       =   roc_auc_score(y_te, y_pr)\n",
    "f1        =        f1_score(y_te, y_pr)\n",
    "precision = precision_score(y_te, y_pr)\n",
    "recall    =    recall_score(y_te, y_pr)\n",
    "\n",
    "\n",
    "# Loop 계산시 성능평가 결과를 리스트로 반환하는 방법\n",
    "l1 = []\n",
    "for idx in range(5):\n",
    "    # 모델 생성 > 훈련 > 예측 과정 수행 #\n",
    "    as1 = accuracy_score(y_te, y_pr)\n",
    "    l1 += [(as1, idx)]\n",
    "r1 = sorted(l1, reverse=True)[0]   # accuray score 기준 내림차순 정렬 후, n번째 값 선택 (최대값은 [0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
